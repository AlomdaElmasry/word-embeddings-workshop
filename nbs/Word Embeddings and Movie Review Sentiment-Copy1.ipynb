{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vectors(loc):\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb'), encoding='latin1'),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb'), encoding='latin1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m6B.100d.dat\u001b[0m/       \u001b[01;34m6B.200d.dat\u001b[0m/       \u001b[01;34m6B.300d.dat\u001b[0m/       \u001b[01;34m6B.50d.dat\u001b[0m/\r\n",
      "6B.100d_idx.pkl    6B.200d_idx.pkl    6B.300d_idx.pkl    6B.50d_idx.pkl\r\n",
      "\u001b[01;31m6B.100d.tgz\u001b[0m        \u001b[01;31m6B.200d.tgz\u001b[0m        \u001b[01;31m6B.300d.tgz\u001b[0m        \u001b[01;31m6B.50d.tgz\u001b[0m\r\n",
      "6B.100d_words.pkl  6B.200d_words.pkl  6B.300d_words.pkl  6B.50d_words.pkl\r\n"
     ]
    }
   ],
   "source": [
    "%ls /data/datasets/nlp/glove/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs, words, wordidx = load_vectors('/data/datasets/nlp/glove/results/6B.300d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['together',\n",
       " 'congress',\n",
       " 'index',\n",
       " 'australia',\n",
       " 'results',\n",
       " 'hard',\n",
       " 'hours',\n",
       " 'land',\n",
       " 'action',\n",
       " 'higher']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[600:610]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordidx allows us to look up a word in order to find out it's index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1226"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordidx['intelligence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'intelligence'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[1226]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What words are similar to intelligence?\n",
    "\n",
    "Right now, our list of words can't answer that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words as vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intelligence is represented by the 100 dimensional vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6.52069986e-01,   2.87869990e-01,  -1.07029997e-01,\n",
       "        -1.06070004e-02,  -2.31639996e-01,  -1.54410005e-01,\n",
       "        -3.81969988e-01,   5.29470026e-01,  -4.03880000e-01,\n",
       "        -3.02410007e+00,   4.64819998e-01,  -5.28949983e-02,\n",
       "        -1.52679995e-01,   3.22320014e-01,  -2.55369991e-01,\n",
       "         2.64629990e-01,   8.37830007e-01,  -9.72450003e-02,\n",
       "         8.51809978e-02,   3.98719996e-01,   1.02660000e-01,\n",
       "        -4.09559995e-01,  -7.27319997e-03,  -3.51529986e-01,\n",
       "        -4.16489989e-01,   6.47870004e-02,  -1.48800001e-01,\n",
       "         7.11059988e-01,  -2.09369995e-02,   5.10909975e-01,\n",
       "        -3.34690005e-01,  -7.14389980e-01,  -4.68459994e-01,\n",
       "         2.97599994e-02,   2.58049995e-01,  -3.82010013e-01,\n",
       "         3.80780011e-01,   3.71329993e-01,  -4.53170002e-01,\n",
       "        -9.43189979e-01,   3.27129990e-01,   1.14349999e-01,\n",
       "        -1.40249997e-01,   2.74509996e-01,  -4.57929999e-01,\n",
       "        -8.18200037e-02,  -3.08620006e-01,   7.91150033e-02,\n",
       "        -9.46670026e-02,   1.64869994e-01,  -1.43390000e-01,\n",
       "         1.34020001e-01,  -5.21170020e-01,  -3.82359996e-02,\n",
       "        -1.87729999e-01,  -1.22660004e-01,  -1.81079999e-01,\n",
       "         2.70260006e-01,   7.13550001e-02,  -1.12020001e-01,\n",
       "        -3.14889997e-01,   8.89620036e-02,   7.43070021e-02,\n",
       "         1.76170006e-01,  -3.56510013e-01,  -8.02330021e-03,\n",
       "         5.78260005e-01,   1.18359998e-01,  -1.70070007e-01,\n",
       "         4.95409995e-01,  -2.63139993e-01,   5.60310006e-01,\n",
       "        -2.08810002e-01,  -3.43459994e-01,  -4.94520009e-01,\n",
       "         3.68489996e-02,  -4.18150015e-02,   1.24190003e-01,\n",
       "        -2.93870002e-01,   3.64650011e-01,   6.25940025e-01,\n",
       "        -6.41060024e-02,   2.11420000e-01,  -1.50920004e-01,\n",
       "         5.92270017e-01,  -3.65750015e-01,  -8.82420018e-02,\n",
       "         4.30409998e-01,  -7.88619965e-02,   2.59240001e-01,\n",
       "        -1.13469994e+00,   6.70499980e-01,  -2.07890004e-01,\n",
       "         4.57060009e-01,  -7.86300004e-02,   2.33480006e-01,\n",
       "        -7.53059983e-01,  -2.12219998e-01,   2.50649989e-01,\n",
       "        -2.13829994e-01,   1.12080000e-01,   1.55460000e-01,\n",
       "         3.27899992e-01,  -2.63429999e-01,   6.01660013e-01,\n",
       "        -6.15949988e-01,  -5.17560005e-01,  -3.76569986e-01,\n",
       "        -1.05949998e-01,   8.90439972e-02,   4.57899988e-01,\n",
       "         4.76999991e-02,   2.21000001e-01,  -9.14420009e-01,\n",
       "         1.30559996e-01,   7.99359977e-01,  -2.27960005e-01,\n",
       "        -3.59899998e-02,   2.44049996e-01,  -2.46950001e-01,\n",
       "        -8.65020007e-02,  -6.78670034e-02,   1.11369997e-01,\n",
       "        -6.05069995e-01,  -2.56370008e-01,  -4.68959987e-01,\n",
       "        -1.84149995e-01,  -5.27279973e-01,  -1.75150007e-01,\n",
       "         4.01910007e-01,  -4.98250008e-01,  -5.21179974e-01,\n",
       "        -2.13740006e-01,  -7.42349997e-02,  -3.10279995e-01,\n",
       "         5.44189990e-01,   1.05769999e-01,  -6.06569983e-02,\n",
       "        -3.00459992e-02,  -3.25650007e-01,   1.43279999e-01,\n",
       "         2.74140000e-01,  -8.43250006e-02,   1.59750000e-01,\n",
       "        -1.32599995e-01,   4.52800006e-01,  -2.71369994e-01,\n",
       "         4.83440012e-02,  -2.98979998e-01,  -4.63910013e-01,\n",
       "         5.47479987e-01,  -4.16900009e-01,   4.20489997e-01,\n",
       "         3.75189990e-01,  -2.80129999e-01,  -7.32299984e-02,\n",
       "         2.06829995e-01,   1.49780005e-01,   6.04640007e-01,\n",
       "        -2.07819998e-01,   6.14349991e-02,   1.98070005e-01,\n",
       "         6.29019976e-01,   7.77899995e-02,   1.27570003e-01,\n",
       "         8.91020000e-02,  -5.13729990e-01,  -1.40880004e-01,\n",
       "         1.12620004e-01,   6.05149984e-01,   2.44029999e-01,\n",
       "        -4.68629986e-01,  -3.01779985e-01,  -5.48640013e-01,\n",
       "        -9.60620027e-03,   2.95280010e-01,  -1.65069997e-01,\n",
       "        -3.10909986e-01,   2.42699996e-01,  -6.14319980e-01,\n",
       "         4.06019986e-02,  -4.19160008e-01,  -2.23120004e-01,\n",
       "         5.55750012e-01,   9.55239981e-02,  -3.74139994e-01,\n",
       "        -1.72780007e-01,  -2.65839994e-01,   6.33490026e-01,\n",
       "        -1.11830004e-01,  -2.47639999e-01,  -5.22350013e-01,\n",
       "        -2.06770003e-01,   2.15770006e-01,  -4.00660008e-01,\n",
       "         5.54489970e-01,   4.65979986e-02,  -2.58340001e-01,\n",
       "        -5.27649999e-01,   3.57569993e-01,   3.56269985e-01,\n",
       "        -1.64930001e-01,   7.99930021e-02,   1.52419999e-01,\n",
       "        -3.70539986e-02,   3.28489989e-01,  -4.19800013e-01,\n",
       "         7.28919983e-01,  -6.98119998e-02,  -7.77220011e-01,\n",
       "         6.07060015e-01,  -4.59300011e-01,   8.67419969e-03,\n",
       "         2.44169995e-01,   5.73090017e-02,  -4.17070001e-01,\n",
       "        -1.91860005e-01,  -8.15970004e-02,   2.05870003e-01,\n",
       "         5.05429983e-01,  -3.23289990e-01,  -5.95480025e-01,\n",
       "         1.18759996e-03,  -1.35820001e-01,  -1.17349997e-01,\n",
       "        -1.27670005e-01,  -6.10029995e-02,  -3.38830017e-02,\n",
       "         5.34829982e-02,   1.36649996e-01,  -4.80660014e-02,\n",
       "        -2.92890012e-01,   3.25659990e-01,  -2.47409999e-01,\n",
       "        -5.60159981e-01,   1.31430000e-01,   2.64820009e-01,\n",
       "        -2.80909985e-01,   1.77100003e-01,   1.81219995e-01,\n",
       "         9.34690014e-02,   1.61950007e-01,   1.07060003e+00,\n",
       "        -4.51220006e-01,   6.04399979e-01,  -6.03980005e-01,\n",
       "        -3.00000012e-01,  -1.55809999e-01,   1.86210006e-01,\n",
       "         4.73560005e-01,  -1.70969993e-01,   3.67349982e-02,\n",
       "         1.34039998e-01,  -1.95189998e-01,   9.04950023e-01,\n",
       "        -3.95429999e-01,   7.66579986e-01,   2.84249991e-01,\n",
       "        -5.84490001e-01,   5.04320025e-01,   3.46219987e-01,\n",
       "         4.81849998e-01,   2.53459997e-03,   1.94739997e-01,\n",
       "         7.40550011e-02,  -8.80620033e-02,  -9.46889967e-02,\n",
       "        -2.26449996e-01,   2.96600014e-01,   2.40419999e-01,\n",
       "        -2.45830007e-02,  -1.77120008e-02,  -6.82099983e-02,\n",
       "         3.42619985e-01,  -3.55089992e-01,   3.05079997e-01,\n",
       "        -1.35490000e+00,   9.15989995e-01,   6.10580027e-01,\n",
       "        -7.18970001e-01,  -3.35700005e-01,   1.79289997e-01,\n",
       "         9.24950004e-01,   5.68290008e-03,   1.08130002e+00,\n",
       "        -5.37289977e-01,   2.76650012e-01,   3.51500005e-01,\n",
       "         4.64700013e-01,   1.16300002e-01,   2.14550003e-01,\n",
       "         8.88120010e-02,  -1.46340001e-02,   8.25600028e-01,\n",
       "        -2.29629993e-01,   7.40379989e-01,  -4.67999995e-01,\n",
       "        -4.85909998e-01,  -9.99509990e-01,  -4.12979990e-01], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs[1226]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lets us do some useful calculations. For instance, we can see how far apart two words are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance between similar words is low:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40639386485700224"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist(vecs[wordidx[\"puppy\"]], vecs[wordidx[\"dog\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36432365465061811"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist(vecs[wordidx[\"queen\"]], vecs[wordidx[\"princess\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And the distance between unrelated words is high:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99318009024356424"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist(vecs[wordidx[\"kitten\"]], vecs[wordidx[\"airplane\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0488782857777486"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist(vecs[wordidx[\"celebrity\"]], vecs[wordidx[\"dusty\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0393029477348061"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist(vecs[wordidx[\"avalanche\"]], vecs[wordidx[\"antique\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see what words are close to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest Neighbors is an algorithm that finds the points closest to a given point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='brute', leaf_size=30, metric='cosine',\n",
       "         metric_params=None, n_jobs=1, n_neighbors=10, p=2, radius=0.5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=10, radius=0.5, metric='cosine', algorithm='brute')\n",
    "neigh.fit(vecs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances, indices = neigh.kneighbors([vecs[1226]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intelligence',\n",
       " 'cia',\n",
       " 'information',\n",
       " 'security',\n",
       " 'counterterrorism',\n",
       " 'operatives',\n",
       " 'fbi',\n",
       " 'military',\n",
       " 'secret',\n",
       " 'spy']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[int(ind)] for ind in indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take this a step further, and add two words together.  What is the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_vec = vecs[wordidx[\"artificial\"]] + vecs[wordidx[\"intelligence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.04946995e+00,   1.09679997e-01,  -2.37800002e-01,\n",
       "         4.41762984e-01,  -6.37489974e-01,  -1.69235006e-01,\n",
       "        -2.28319988e-01,   1.35890037e-01,  -2.92060018e-01,\n",
       "        -4.89580011e+00,   7.30489969e-01,  -9.51889977e-02,\n",
       "        -7.50439912e-02,   1.99990004e-01,   2.74540037e-01,\n",
       "         3.27181995e-01,   6.39230013e-01,   2.27394998e-01,\n",
       "        -2.37269014e-01,   7.63599992e-01,  -3.67049992e-01,\n",
       "        -2.92549998e-01,  -6.74103200e-01,   4.04150039e-01,\n",
       "        -1.12989998e+00,   3.89737010e-01,  -4.74629998e-01,\n",
       "         4.07639980e-01,  -3.42967004e-01,   1.52470994e+00,\n",
       "        -3.64854008e-01,  -3.87269974e-01,  -9.44000006e-01,\n",
       "         9.67870057e-02,   5.07059991e-01,   5.33720016e-01,\n",
       "         9.79380012e-01,   3.42716992e-01,  -1.99719995e-01,\n",
       "        -4.25859988e-01,  -1.31960005e-01,   2.58089989e-01,\n",
       "         1.28430009e-01,   5.97499967e-01,  -7.00890005e-01,\n",
       "        -3.12350005e-01,  -1.79130003e-01,   7.68934965e-01,\n",
       "         4.79852974e-01,   9.45969939e-01,   7.85000622e-03,\n",
       "         8.03359985e-01,  -8.54990005e-01,  -1.59135997e-01,\n",
       "        -9.44830000e-01,   1.18620001e-01,  -2.59842008e-01,\n",
       "         3.85960013e-01,   1.86385006e-01,   8.50849986e-01,\n",
       "        -7.48809993e-01,   7.47470036e-02,  -1.11233003e-01,\n",
       "        -1.24389991e-01,  -1.62760019e-01,   2.95476705e-01,\n",
       "         8.10829997e-01,   6.44909978e-01,  -9.43510085e-02,\n",
       "         9.41740036e-01,  -8.26099962e-02,   1.02722001e+00,\n",
       "        -4.60379988e-01,  -4.70389992e-01,  -4.96873617e-01,\n",
       "         3.64868999e-01,   8.22449923e-02,   1.08823001e-01,\n",
       "         3.17649990e-01,   3.80232006e-01,   1.05979002e+00,\n",
       "        -3.33595991e-01,  -2.58630008e-01,  -5.31340003e-01,\n",
       "         8.00700188e-02,  -4.56151009e-01,   1.73158020e-01,\n",
       "         6.81900024e-01,   1.24257997e-01,   4.22869980e-01,\n",
       "        -1.58690000e+00,   8.82629991e-01,  -2.91574001e-01,\n",
       "         6.44410014e-01,   1.18300006e-01,   7.76489973e-01,\n",
       "        -6.34639978e-01,  -6.23560011e-01,   3.54879975e-01,\n",
       "        -1.03899002e+00,   6.00499988e-01,   6.35500014e-01,\n",
       "         1.44970000e-01,  -6.25299960e-02,   1.04919004e+00,\n",
       "        -1.62019998e-01,  -1.06520998e+00,  -6.87849998e-01,\n",
       "        -7.04069972e-01,  -3.17505985e-01,   1.42470002e-01,\n",
       "         1.74740002e-01,   1.18749999e-01,  -1.03101003e+00,\n",
       "        -4.44100052e-02,   8.33127975e-01,   2.60799974e-02,\n",
       "         4.46449995e-01,  -1.00019991e-01,  -2.64299959e-02,\n",
       "        -1.74890012e-01,   1.91192985e-01,   3.74700010e-01,\n",
       "        -2.56370008e-01,   1.83299780e-02,  -1.12102997e+00,\n",
       "        -9.07619968e-02,  -6.61949992e-01,  -1.13524005e-01,\n",
       "         3.47561002e-01,  -5.20556986e-01,  -2.22699642e-02,\n",
       "        -1.61974013e-01,   7.94234991e-01,  -5.85340023e-01,\n",
       "         8.62959981e-01,   2.15590000e-01,  -4.41567004e-01,\n",
       "        -4.85135972e-01,  -4.57170010e-01,   4.88310009e-01,\n",
       "         1.61320001e-01,  -1.70677006e-01,  -3.94710004e-01,\n",
       "         2.29850009e-01,   7.44369984e-01,  -6.43299967e-02,\n",
       "        -5.71455956e-01,  -3.44725996e-01,  -8.22510004e-01,\n",
       "        -5.37199974e-02,  -1.00041997e+00,  -1.94199979e-02,\n",
       "         6.75830007e-01,  -2.37841994e-01,  -1.25728995e-01,\n",
       "        -2.60360003e-01,   3.18650007e-01,   4.08520013e-01,\n",
       "         5.23900092e-02,   2.58904994e-01,  -2.69000232e-03,\n",
       "         2.80659974e-01,   3.33929986e-01,   5.33120036e-01,\n",
       "         3.64032000e-01,  -2.42829978e-01,   2.29019985e-01,\n",
       "         2.48919994e-01,   4.68359977e-01,   6.13899976e-02,\n",
       "        -3.16799998e-01,  -2.74217993e-01,  -7.72480011e-01,\n",
       "         2.01003805e-01,   3.81817013e-01,  -5.86099997e-02,\n",
       "         2.49909997e-01,  -6.72950029e-01,  -9.87789989e-01,\n",
       "         4.52921987e-01,  -4.62553024e-01,  -1.77374005e-01,\n",
       "         9.45880055e-01,   5.11219986e-02,  -2.87474990e-01,\n",
       "         1.76739991e-01,  -7.64189959e-01,   8.25330019e-01,\n",
       "        -8.99489999e-01,  -2.15100050e-02,  -3.16630006e-01,\n",
       "         7.76900053e-02,  -1.23899877e-02,  -1.03867996e+00,\n",
       "         8.04690003e-01,   2.97089983e-02,  -7.47130036e-01,\n",
       "        -4.63526011e-01,   1.38097000e+00,   7.39629984e-01,\n",
       "        -4.00860012e-01,  -2.01386988e-01,  -5.84799945e-02,\n",
       "        -4.91254002e-01,   7.05349982e-01,  -1.01969004e+00,\n",
       "         1.27131999e+00,  -1.96232006e-01,  -7.24141002e-01,\n",
       "         9.16350007e-01,  -5.20713031e-01,   3.76944214e-01,\n",
       "        -4.02220011e-01,   1.84889004e-01,  -2.57610023e-01,\n",
       "        -1.34944007e-01,  -4.75546986e-01,   8.01249981e-01,\n",
       "         2.86809981e-01,  -6.31999969e-03,  -6.77706003e-01,\n",
       "         1.57737598e-01,  -6.36870027e-01,  -1.21625498e-01,\n",
       "        -4.38080013e-01,   2.13380009e-02,   6.32087052e-01,\n",
       "         3.55773002e-01,   2.82769978e-01,   1.20903397e+00,\n",
       "         2.42799819e-02,   5.76879978e-01,  -4.46390003e-01,\n",
       "        -1.42487001e+00,   6.34999573e-03,   2.29268014e-01,\n",
       "        -2.80083925e-01,  -9.49200094e-02,  -1.83499992e-01,\n",
       "         2.84249008e-01,  -2.71390021e-01,   9.50460017e-01,\n",
       "        -1.88560009e-01,  -6.47900045e-01,  -9.39390004e-01,\n",
       "        -2.97250509e-01,   5.20410001e-01,   5.33400029e-02,\n",
       "        -1.89779967e-01,  -1.72199905e-02,   1.27673000e-01,\n",
       "         5.71539998e-01,  -2.72713989e-01,   8.73374999e-01,\n",
       "        -4.31665987e-01,   9.12840009e-01,  -2.56790012e-01,\n",
       "        -2.95399994e-01,   2.29710013e-01,   1.60179988e-01,\n",
       "        -9.92399752e-02,  -4.02945399e-01,   9.62539986e-02,\n",
       "         5.45835018e-01,   2.19479948e-02,  -4.71669018e-01,\n",
       "        -4.24269974e-01,   8.26160073e-01,   4.46240008e-01,\n",
       "        -1.50062993e-01,   1.29348010e-01,  -4.33950007e-01,\n",
       "        -2.59400040e-01,  -6.24949992e-01,   4.19079989e-01,\n",
       "        -2.53429985e+00,   1.19388998e+00,   1.46840036e-01,\n",
       "        -1.02135003e+00,  -5.45750022e-01,  -9.84399915e-02,\n",
       "         1.37282002e+00,   4.36289012e-02,   9.53490019e-01,\n",
       "        -8.37459981e-01,   6.21800125e-02,  -8.59299898e-02,\n",
       "         7.38759995e-01,   6.41000271e-03,  -5.94000071e-02,\n",
       "        -4.83480021e-02,   1.49639994e-02,   1.14805007e+00,\n",
       "        -4.80569988e-01,   4.49779987e-01,  -1.93489999e-01,\n",
       "        -5.47150016e-01,  -7.23600030e-01,  -3.92199755e-02], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhoward/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "distances, indices = neigh.kneighbors(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intelligence',\n",
       " 'artificial',\n",
       " 'information',\n",
       " 'knowledge',\n",
       " 'cia',\n",
       " 'methods',\n",
       " 'secret',\n",
       " 'source',\n",
       " 'capabilities',\n",
       " 'sources']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[int(ind)] for ind in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhoward/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "distances, indices = neigh.kneighbors(vecs[wordidx[\"kitten\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kitten',\n",
       " 'kittens',\n",
       " 'puppy',\n",
       " 'puppies',\n",
       " 'pooch',\n",
       " 'cat',\n",
       " 'cute',\n",
       " 'purr',\n",
       " 'adorable',\n",
       " 'rottweiler']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[int(ind)] for ind in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_vec = vecs[wordidx[\"kitten\"]] - vecs[wordidx[\"cat\"]] + vecs[wordidx[\"dog\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances, indices = neigh.kneighbors([new_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kitten',\n",
       " 'puppy',\n",
       " 'dog',\n",
       " 'rottweiler',\n",
       " 'dogs',\n",
       " 'puppies',\n",
       " 'retriever',\n",
       " 'leash',\n",
       " 'hound',\n",
       " 'pooch']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[int(ind)] for ind in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances, indices = neigh.kneighbors([vecs[wordidx[\"king\"]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king',\n",
       " 'queen',\n",
       " 'prince',\n",
       " 'monarch',\n",
       " 'kingdom',\n",
       " 'throne',\n",
       " 'ii',\n",
       " 'iii',\n",
       " 'crown',\n",
       " 'reign']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[int(ind)] for ind in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_vec = vecs[wordidx[\"king\"]] - vecs[wordidx[\"man\"]] + vecs[wordidx[\"woman\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances, indices = neigh.kneighbors([new_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king',\n",
       " 'queen',\n",
       " 'monarch',\n",
       " 'throne',\n",
       " 'princess',\n",
       " 'mother',\n",
       " 'daughter',\n",
       " 'kingdom',\n",
       " 'prince',\n",
       " 'elizabeth']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[int(ind)] for ind in indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors  d=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "         metric_params=None, n_jobs=1, n_neighbors=5, p=2, radius=0.5)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=5, radius=0.5)\n",
    "neigh.fit(vecs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances, indices = neigh.kneighbors([vecs[wordidx[\"queen\"]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['queen', 'princess', 'lady', 'elizabeth', 'prince']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[int(ind)] for ind in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances, indices = neigh.kneighbors([vecs[wordidx[\"tarantula\"]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tarantula', 'two-headed', 'leviathan', 'rattler', 'ape']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[int(ind)] for ind in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_vec = vecs[wordidx[\"kitten\"]] - vecs[wordidx[\"cat\"]] + vecs[wordidx[\"dog\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances, indices = neigh.kneighbors([new_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34698, 22454, 76671,  2926, 54331]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kitten', 'puppy', 'rottweiler', 'dog', 'spunky']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[int(ind)] for ind in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.149688"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(vecs[wordidx[\"puppy\"]] - vecs[wordidx[\"dog\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0129473"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(vecs[wordidx[\"queen\"]] - vecs[wordidx[\"princess\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.3257985"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(vecs[wordidx[\"kitten\"]] - vecs[wordidx[\"airplane\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.8440499"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(vecs[wordidx[\"celebrity\"]] - vecs[wordidx[\"dusty\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.6188631"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(vecs[wordidx[\"avalanche\"]] - vecs[wordidx[\"antique\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to look at the IMDB dataset, which contains movie reviews from IMDB, along with their sentiment. Keras comes with some helpers for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.utils.data_utils import get_file\n",
    "idx = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the word list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_arr = sorted(idx, key=idx.get)\n",
    "idx_arr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and this is the mapping from id to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx2word = {v: k for k, v in idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the reviews using code copied from keras.datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = get_file('imdb_full.pkl',\n",
    "                origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n",
    "                md5_hash='d091312047c43cf9e4e38fef92437263')\n",
    "f = open(path, 'rb')\n",
    "(x_train, labels_train), (x_test, labels_test) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the 1st review. As you see, the words have been replaced by ids. The ids can be looked up in idx2word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23022, 309, 6, 3, 1069, 209, 9, 2175, 30, 1, 169, 55, 14, 46, 82, 5869, 41, 393, 110, 138, 14, 5359, 58, 4477, 150, 8, 1, 5032, 5948, 482, 69, 5, 261, 12, 23022, 73935, 2003, 6, 73, 2436, 5, 632, 71, 6, 5359, 1, 25279, 5, 2004, 10471, 1, 5941, 1534, 34, 67, 64, 205, 140, 65, 1232, 63526, 21145, 1, 49265, 4, 1, 223, 901, 29, 3024, 69, 4, 1, 5863, 10, 694, 2, 65, 1534, 51, 10, 216, 1, 387, 8, 60, 3, 1472, 3724, 802, 5, 3521, 177, 1, 393, 10, 1238, 14030, 30, 309, 3, 353, 344, 2989, 143, 130, 5, 7804, 28, 4, 126, 5359, 1472, 2375, 5, 23022, 309, 10, 532, 12, 108, 1470, 4, 58, 556, 101, 12, 23022, 309, 6, 227, 4187, 48, 3, 2237, 12, 9, 215'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(map(str, x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first word of the first review is 23022. Let's see what that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[23022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the whole review, mapped from ids to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell high's satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers' pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i'm here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isn't\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[o] for o in x_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are 1 for positive, 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce vocab size by setting rare words to max index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "\n",
    "trn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_train]\n",
    "test = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at distribution of lengths of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([4999,  309,    6,    3, 1069,  209,    9, 2175,   30,    1,  169,\n",
       "          55,   14,   46,   82, 4999,   41,  393,  110,  138,   14, 4999,\n",
       "          58, 4477,  150,    8,    1, 4999, 4999,  482,   69,    5,  261,\n",
       "          12, 4999, 4999, 2003,    6,   73, 2436,    5,  632,   71,    6,\n",
       "        4999,    1, 4999,    5, 2004, 4999,    1, 4999, 1534,   34,   67,\n",
       "          64,  205,  140,   65, 1232, 4999, 4999,    1, 4999,    4,    1,\n",
       "         223,  901,   29, 3024,   69,    4,    1, 4999,   10,  694,    2,\n",
       "          65, 1534,   51,   10,  216,    1,  387,    8,   60,    3, 1472,\n",
       "        3724,  802,    5, 3521,  177,    1,  393,   10, 1238, 4999,   30,\n",
       "         309,    3,  353,  344, 2989,  143,  130,    5, 4999,   28,    4,\n",
       "         126, 4999, 1472, 2375,    5, 4999,  309,   10,  532,   12,  108,\n",
       "        1470,    4,   58,  556,  101,   12, 4999,  309,    6,  227, 4187,\n",
       "          48,    3, 2237,   12,    9,  215]),\n",
       " array([4999,   39, 4999,   14,  739, 4999, 3428,   44,   74,   32, 1831,\n",
       "          15,  150,   18,  112,    3, 1344,    5,  336,  145,   20,    1,\n",
       "         887,   12,   68,  277, 1189,  403,   34,  119,  282,   36,  167,\n",
       "           5,  393,  154,   39, 2299,   15,    1,  548,   88,   81,  101,\n",
       "           4,    1, 3273,   14,   40,    3,  413, 1200,  134, 4999,   41,\n",
       "         180,  138,   14, 3086,    1,  322,   20, 4930, 4999,  359,    5,\n",
       "        3112, 2128,    1, 4999, 4999,   39, 4999,   45, 3661,   27,  372,\n",
       "           5,  127,   53,   20,    1, 1983,    7,    7,   18,   48,   45,\n",
       "          22,   68,  345,    3, 2131,    5,  409,   20,    1, 1983,   15,\n",
       "           3, 3238,  206,    1, 4999,   22,  277,   66,   36,    3,  341,\n",
       "           1,  719,  729,    3, 3865, 1265,   20,    1, 1510,    3, 1219,\n",
       "           2,  282,   22,  277, 2525,    5,   64,   48,   42,   37,    5,\n",
       "          27, 3273,   12,    6, 4999, 4999, 2034,    7,    7, 3771, 3225,\n",
       "          34, 4186,   34,  378,   14, 4999,  296,    3, 1023,  129,   34,\n",
       "          44,  282,    8,    1,  179,  363, 4999,    5,   94,    3, 2131,\n",
       "          16,    3, 4999, 3005, 4999, 4999,    5,   64,   45,   26,   67,\n",
       "         409,    8,    1, 1983,   15, 3261,  501,  206,    1, 4999,   45,\n",
       "        4999, 2877,   26,   67,   78,   48,   26,  491,   16,    3,  702,\n",
       "        1184,    4,  228,   50, 4505,    1, 4999,   20,  118, 4999,    6,\n",
       "        1373,   20,    1,  887,   16,    3, 4999,   20,   24, 3964,    5,\n",
       "        4999,   24,  172,  844,  118,   26,  188, 1488,  122,    1, 4999,\n",
       "         237,  345,    1, 4999, 4999,   31,    3, 4999,  100,   42,  395,\n",
       "          20,   24, 4999,  118, 4999,  889,   82,  102,  584,    3,  252,\n",
       "          31,    1,  400,    4, 4787, 4999, 1962, 3861,   32, 1230, 3186,\n",
       "          34,  185, 4310,  156, 2325,   38,  341,    2,   38, 4999, 4999,\n",
       "        2231, 4846,    2, 4999, 4999, 2610,   34,   23,  457,  340,    5,\n",
       "           1, 1983,  504, 4355, 4999,  215,  237,   21,  340,    5, 4468,\n",
       "        4999, 4999,   37,   26,  277,  119,   51,  109, 1023,  118,   42,\n",
       "         545,   39, 2814,  513,   39,   27,  553,    7,    7,  134,    1,\n",
       "         116, 2022,  197, 4787,    2, 4999,  283, 1667,    5,  111,   10,\n",
       "         255,  110, 4382,    5,   27,   28,    4, 3771, 4999, 4999,  105,\n",
       "         118, 2597,    5,  109,    3,  209,    9,  284,    3, 4325,  496,\n",
       "        1076,    5,   24, 2761,  154,  138,   14, 4999, 4999,  182, 4999,\n",
       "          39, 4999,   15,    1,  548,    5,  120,   48,   42,   37,  257,\n",
       "         139, 4530,  156, 2325,    9,    1,  372,  248,   39,   20,    1,\n",
       "          82,  505,  228,    3,  376, 2131,   37,   29, 1023,   81,   78,\n",
       "          51,   33,   89,  121,   48,    5,   78,   16,   65,  275,  276,\n",
       "          33,  141,  199,    9,    5,    1, 3273,  302,    4,  769,    9,\n",
       "          37, 4999,  275,    7,    7,   39,  276,   11,   19,   77, 4999,\n",
       "          22,    5,  336,  406]),\n",
       " array([ 527,  117,  113,   31, 4999, 1962, 3861,  115,  902, 4999,  758,\n",
       "          10,   25,  123,  107,    2,  116,  136,    8, 1646, 4999,   23,\n",
       "         330,    5,  597,    1, 4999,   20,  390,    6,    3,  353,   14,\n",
       "          49,   14,  230,    8, 4999, 4999,    1,  190,   20, 4999,    6,\n",
       "          79,  894,  100,  109, 3609,    4,  109,    3, 4999, 3485,   43,\n",
       "          24, 1407,    2,  109, 4999,    1, 2405,    4, 4999, 4999, 4999,\n",
       "        4999,  143,    3, 2405,   26,  557,  286,  160,  712, 4122, 4999,\n",
       "           3,  511,   36,    1,  300, 2793, 4999,  120,    6,  774,  130,\n",
       "          96,   14,    3, 1165, 4999,   34,  491,    5, 4263,    1, 4999,\n",
       "          24,  106,    6,   50, 4999,   71,  641,    1, 1547,  133,    2,\n",
       "           1,  133,  118,    1, 3273, 4999,    3, 4999, 2135,   23,   29,\n",
       "          55, 2236,  165,   15,    1, 2974,  133,    2,    1,  104,  191,\n",
       "        4999,  994,   28, 4999,   11,   17,  211,  125,  254,   55,   10,\n",
       "          64,    9,   60,    6,  176,  397]),\n",
       " array([  11,    6,  711,    1,   88, 2181,   19, 4999,    1, 3225, 4999,\n",
       "         249,   91, 3045,    9,  124,   21,  199,    3,  818,  647,    4,\n",
       "        4999, 1022,  132,   86, 3842, 3558,  517,    3,  818,  647,    4,\n",
       "        4999, 4999,   39, 2743,  517,    3,  818,  647,    4, 4999,   22,\n",
       "        3752,  108,    4,    1,  637,  806, 1032,   18,  128,   11,   19,\n",
       "           6,   52, 3201,    8,    3,   93,  108, 1287,   23,   21,    2,\n",
       "           5, 1595,   12,  122,    8,    3,   62,   41,   46,    4,    1,\n",
       "          88, 4999, 4999, 1063,    4,  923,    6,  368, 1156,   91,   21,\n",
       "           1, 3870,  708,   18,   91,   21,  592,  342,   58,   61, 3303,\n",
       "           6,   12, 3225,  141,   25,  174,  291,  331,    8,    1,  482,\n",
       "          10,  116, 3771,   14,    3,  164,    2,  561,   21,   35,   73,\n",
       "          14,    3,  482]),\n",
       " array([  11,    6,   21,    1,  798, 3771, 3225,   19,    9,   13,   73,\n",
       "         326, 2761,   71,   88,    4,   24,   99,    2,  162,   66,    3,\n",
       "         111,   12,   13, 4999, 2811, 1962, 3861,   90,    1,   17,   56,\n",
       "           6,  138,    3,  774,  464, 1147,  521,   47,   68,   46,  385,\n",
       "          12,   97,   25,   74, 4999,   43,    3,  224,   50,    2,   46,\n",
       "         136,   12,   97,  239,   25,   74,  602,    5,   94,    1,  670,\n",
       "           5,   78,   35,   18,   29,    8,   29,   11,    6,  287,    1,\n",
       "        1863,    5,  848,    2,   64,    9,    1,  113,   13,   49,  441,\n",
       "        3225,  306,  119,    3,   49,  289,  206,   24, 4999, 1383,    5,\n",
       "        2552,    5,    1,  308,  171, 3861,   13,    1,  115,  281,    8,\n",
       "           1,   17,   18, 4999,    2, 4999,  196,  253,   65,  528,   70]),\n",
       " array([  11,  215,    1, 1714, 2160, 1698,  882,    6,    9,    1, 2809,\n",
       "        2137, 2160, 1698,    4, 1133,  705, 2243,   11,    6,    3, 4999,\n",
       "           4,    1,  353,  450,  206,  117, 4999, 1846,   16, 4999,  159,\n",
       "         116,    4,    1,  705,   18,   11,  215,    3,  705, 3110, 4999,\n",
       "          11,    6,   50,    3,  733,  833, 2193,  140,   60, 1698, 1024,\n",
       "           5, 4999,    3, 1192,  427,    2,   24, 4999,    7,    7,   79,\n",
       "        1181, 4602,  446,    2, 4999, 4999,   11,  833,  450,  296,  181,\n",
       "          73,   37,    3, 1633, 4433,  363, 4999,  106,  211,  488,    5,\n",
       "        4999,   24, 3356,    7,    7,   10,  212,  132,   12,   10,   13,\n",
       "         542, 2173,  148,   11,   17,  993,    5, 3333, 3616, 4999,   39,\n",
       "        4999,    9,  418,   50,   37,   10,   13,  146,    3,  229, 1698,\n",
       "          14,   26,   13,  162, 3459,    1, 1726,   36,    3,  837,  412,\n",
       "        1968,    8,   82,  712,    9,  418,  144,    2,   10,   13,  499,\n",
       "           5, 4999,    5,    1,  860,    4,    1,   62,    7,    7,   29,\n",
       "           8,   29,   42,  287,    3,  103,  148,   42,  404,   21, 2550,\n",
       "        2321,  311, 2393,    7,    7,    9, 4999,    3,  690,  690,  155,\n",
       "          36,    7,    7,    1, 4999]),\n",
       " array([ 419,   91,   32,  495,    5, 2973,   94,    3,  547, 1782,  705,\n",
       "           7,    7,    1,   62, 4183,    8,  324, 4999,  134,   22,   89,\n",
       "          57, 1492,    9, 1445,    7,    7,  475,  236,   31, 2160, 1698,\n",
       "           1, 3121, 2442,    8,    1,   19,   67,  303, 1741,    2,   67,\n",
       "         239, 4522,   86,   73,   22,  355,    1,   19,  187,    1, 2023,\n",
       "         111,    6,   52, 1725,    1,   17,  149, 3406, 1643,   22,    2,\n",
       "         128, 4999,   22,  192,    5,  398,   22, 1532,    1,  455,    6,\n",
       "          49,  358,    4, 2687,    5, 2712, 4627, 4999,    4,  833,    2,\n",
       "        4999,    6,   49,    7,    7,   52,  324,  297,   55,  103,   45,\n",
       "          22,   23,  264,    5, 4582,  142,    2,  839,    3, 3014,  343,\n",
       "          62]),\n",
       " array([   8,   11, 4999, 4999, 1984,  705,  445,   20,  280,  684, 4868,\n",
       "        2160, 1698,    3, 4999,  561,    2,  519,  311,  737,  120, 3229,\n",
       "         458, 4999,   31,    1, 4999,   62,    4,    3,  182, 4999,    2,\n",
       "          24, 4999,  449, 4999, 4999,   51, 4999, 1201, 4999,   41,   11,\n",
       "        4999,   62,  187, 4868,  656,  306, 1306,   80,    3, 4999,  733,\n",
       "          12, 4999,    3, 2492, 4999, 1790,    5,  595, 4116, 3932,    7,\n",
       "           7,   22,   63,  141,  567,  883,  131,  792,    2,  103,    1,\n",
       "          19,  147,    7,    7,    1,   86,  119,   26, 1582,   24, 3964,\n",
       "         274,   16, 1560, 4999, 3598,   38,  159,  110,  141,   27, 4999,\n",
       "         122,    2, 1409,    5, 4999,  136, 1538,   42, 4999,    1,  280,\n",
       "         873,    4,   38, 1745,    2, 1749, 4999,  141,   27,  575,   31,\n",
       "           1,   55,  440, 1698, 1744,    5,  159,  779,  866,   38, 4999,\n",
       "          97,   27,    8,  885,   18,    3, 3408,   97,   25,   27,   90,\n",
       "         810,    8,  342,    1, 4999,   39,  371, 2211,  136,    1,   19,\n",
       "          59, 4272,   36,    3,  793,  799,   86,   41,    3, 2027,  602,\n",
       "           7,    7, 1698,    2, 3468, 4999,   14, 4999,   89,  303, 2718,\n",
       "         864,   14,    3,  375,    3,  133,   39,  104, 4999,   65,  646,\n",
       "         235,   25, 1675,  267,    1,  865,  897,    1,  174,    6, 4999,\n",
       "        1698, 1577,   32, 4840,  562, 3585,    2,   21,    3,  989, 4999,\n",
       "        4602,  446,   14, 2244,  911, 4999,   14, 4999,    2, 4999, 4999,\n",
       "        4999, 4999,   23,   29,  401,    7,    7,  115,    4,   29, 4999,\n",
       "        4204, 3243,    8,    1,  945, 2367,    4, 2243, 1560,  446,    6,\n",
       "        2293,    8,  657, 4999,  235,   27,   22,  121,   37,   12,  229,\n",
       "          36, 4999,   47,   25,   74,  447,  150,   51, 4999,  740,  113,\n",
       "        2125,  465,    5, 2098,   15,  369,  685,    5,    3, 4999, 4999,\n",
       "           4,  552,  431,   33,   97,   25, 1922, 4999,   16,   46, 1341,\n",
       "        4999,   56,    6,   12,   49,    2,  164, 2326, 4999,  404, 4999,\n",
       "        2909,   26,   57,  163,  394,    3, 4999,   36,    3, 4999, 1689,\n",
       "        2567,    7,    7,  414,  924, 4999, 4999, 4999,    2, 3970, 2545,\n",
       "        1830, 4999,   36, 2814, 4999, 2584,    7,    7,    1,  311, 4999,\n",
       "         297, 4999, 4999, 2326, 4999, 2160, 1698, 4999, 4999, 4602,  446,\n",
       "        4999, 4999]),\n",
       " array([   1,  311, 4999, 2942,  297,  238, 2160, 1698, 4999, 4999, 3468,\n",
       "        4999, 4999, 4999,  911, 4999, 4602,  446,  305, 4999, 2748, 4999,\n",
       "        4999, 1962, 3368, 4999, 2326, 4999,    7,    7, 4999, 4999,  405,\n",
       "        1698,    3,  756,   43,  361, 1314,  236,    7,    7,   48,    6,\n",
       "           9,   41, 4999,    2,  448,   48,    6,    1,  748, 4578,   28,\n",
       "        4999,   16,    1,   82,    2,  135,    6,    9,  217,    1, 4999,\n",
       "           7,    7,    8,    1, 2481, 4999,  334, 2675,  445,   20,  280,\n",
       "         684,   54,  326, 1698,  378,   14,    3,  737, 1875, 1610,  770,\n",
       "        4868,   54,   28,   34, 4532,  534,  237, 4999,  117,    1, 4999,\n",
       "           2,   44, 4999,   32,  218,  334,    8,    1,  809,    4,    3,\n",
       "         182,  427,  770, 4999, 4999, 4999,   34,   44, 4999,    3, 4999,\n",
       "          41,    1, 4999,    4,   24, 3203, 1937,    5,   54, 2063, 3791,\n",
       "        4999, 4999,   34,  405,    9,    5,   54,   28,    5,  329,   15,\n",
       "         306,    7,    7,   54,   28,    6, 1955, 4011,   18, 1113, 3701,\n",
       "          41,    1, 4999, 2008,    4, 4999,  109, 4999,    2, 3372, 4999,\n",
       "          15,  150,  363,   26,   13,  414, 4999,   31,    3, 3730,  770,\n",
       "        4204, 4999,  740,   32,  318,  236,   34,   44, 4999,    1,  427,\n",
       "          18,   38, 4999,   16,   54,   28, 2669,   12, 4999,    6, 1718,\n",
       "          36, 4419, 1955,   54,   28,  491,    5,  906,    1,  448,   18,\n",
       "           6, 1084,    8,  821,    5,   65,  866, 4999, 4999, 4201,   51,\n",
       "           1, 4446,    6, 4999,   31,   24, 4999, 1458, 4999, 4999,  622,\n",
       "        2114, 4999,   36,   65,  159,  779,  540, 1600,   44,   54,   28,\n",
       "           8,   32,  918, 4999,   12,   44,   61,  147, 2068,   80,    3,\n",
       "        4999,    8,    3, 4999,   51,   26, 1065,    5,   78,   46, 4999,\n",
       "          80, 4204,    2, 4999, 4257, 4999,   46, 4999,   12,   26,  158,\n",
       "        4999,    7,    7,  395,   31, 4999, 4999,   34,  998, 1037,    1,\n",
       "         878,   16,   24, 1135, 1458, 3970, 2545,    2,    1,  595, 4999,\n",
       "         164, 4999,    2,  445,   20,    3,  280,   62,   41,    3, 4999,\n",
       "        4999,  255,   43,   44,   46, 4999,  385,   12,  518,   20,  365,\n",
       "        4999,   37,   98,   49,  151, 3354, 4357, 4999,  124,    9, 1522,\n",
       "          12, 1698,  405,    3,  756,   43,  361, 1314,  236,   14,    1,\n",
       "        4999,   49, 2284, 1610,   34, 2067,  491,    5,  261,   12,   24,\n",
       "         609,   28,  334,    6,    8,  189,  144,    2,  124,  116,   87,\n",
       "           1,   28,  152,   12,   44, 3951,   24,  202,  632,    2,   44,\n",
       "          46, 4330, 2147,  385,   16,    1,  945, 4999,  622,   28, 1745,\n",
       "        4999,   10,   77,  560, 4999,   18, 4999,    1, 4179,    4,   38,\n",
       "         106,   12,   67, 4999,   22,    5,    1, 2023,    7,    7,  187,\n",
       "           1,   19, 1126,   43,    4, 2548,    2,  850,  458,    3,  224,\n",
       "        3608,    2,  724,  463,    3, 4999,  523,  415,    4, 4999,    2,\n",
       "         733,   31, 4999,    9, 4101,    5, 1629,    5,  126,  202, 2416,\n",
       "         541,   27, 4687,    4,   48,   22,  437,   15]),\n",
       " array([  22,  121, 2160, 1698,  555, 4999,   87,    6, 1340, 1202,  306,\n",
       "           8,    1, 2017, 4438,   16,   29,  131,  992, 1287,   26,   44,\n",
       "         221,   11, 2065,   16,  379,    1, 1398,    4,  338,    5, 4999,\n",
       "          60, 4999,   51,    9,  382,   43,   18,    6,  147,    3, 1212,\n",
       "         353,    1, 3284,   26,   44,   90, 4438,   25,   74,  774,  259,\n",
       "        4999,    2,   28,  531, 4880,    1,  311, 4999,  463, 1498,  854,\n",
       "           2,    3, 1602,  285,  763,    6,  790,   24,  115,  154,  807,\n",
       "           7,    7,   11,    6,    3,   52, 2857,   62,   57,  148,    9,\n",
       "         149, 1467,    3, 1432,  452,   39,  256,   12, 3102, 1821,   15,\n",
       "          12,  548,    1, 1117,    4,    1,   19,    6,  445,   20,   32,\n",
       "         776,  417,    4, 4999,   12,  128,   44,  243,    5,   27, 4999,\n",
       "        4999,    8,  309,  393,   10,  329,   32, 4999,   31,    3,  503,\n",
       "         770, 2044, 4999, 2817,   34, 3114, 2990, 2566,    2,  850, 4999,\n",
       "        4419,   14,    3,  956,   10,   13, 1678,   31,    1,   62,  363,\n",
       "          10,  329, 4999, 4689,   12, 2817,  200,   21,  162, 1775,   51,\n",
       "          10,  216,   11,   17,    1, 1468, 1414,   12, 2160, 1698,   35,\n",
       "        2102,  997, 4999,    8,   58,  327,    7,    7, 4999, 4999,  239,\n",
       "         405,   38,  115,  902,  236,   96,   14,    1, 1113, 4999, 4999,\n",
       "          38,  214,   13,    3,  227, 1412,   36,  145,   56,   66,    8,\n",
       "          99,   37,  114,  714, 3903,   47,   68,   57,  208,   56,  607,\n",
       "          80,    1,  367,  118,   10,  194,   56,   13, 4485,  205,   30,\n",
       "          69,    9,  301,    3,   49,  521,    5,  294,   12,  429,    4,\n",
       "         214,    2,   42,   11, 4635,  243,   70, 4999,  214,   12,  163,\n",
       "        4999, 4999,  239,   28,    4,    1,  115, 1504,    4,   11, 2245,\n",
       "          21,    5,   25,   57,   74, 2302,   15,   32, 1806, 1341,   14,\n",
       "           4, 4999,   42, 1045,   12,   47,    6,   30,  219,   28,  252,\n",
       "           8,   11,  179,   34,    6,   37,   11,    2,   42,  626,   96,\n",
       "           7,    7,   11,    6,    3,   49,  462,   19,   12,   10,  542,\n",
       "         383,   27, 2845,    5,   27, 4999,  148,   85,   11,   17,  886,\n",
       "          22,   16,    3,  677,  544,   30,    1,  127])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lens = np.array([len(review) for review in trn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2493, 10, 237.71364)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad (with zero) or truncate each sentence to make consistent length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 500\n",
    "\n",
    "trn = sequence.pad_sequences(trn, maxlen=seq_len, value=0)\n",
    "test = sequence.pad_sequences(test, maxlen=seq_len, value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in nice rectangular matrices that can be passed to ML algorithms. Reviews shorter than 500 words are pre-padded with zeros, those greater are truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [stanford paper](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf) that this dataset is from cites a state of the art accuracy (without unlabelled data) of 0.883. So we're short of that, but on the right track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The glove word ids and imdb word ids use different indexes. So we create a simple function that creates an embedding matrix using the indexes from imdb, and the embeddings from glove (where they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_emb():\n",
    "    n_fact = vecs.shape[1]\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "\n",
    "    for i in range(1,len(emb)):\n",
    "        word = idx2word[i]\n",
    "        if word and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
    "            src_idx = wordidx[word]\n",
    "            emb[i] = vecs[src_idx]\n",
    "        else:\n",
    "            # If we can't find the word in glove, randomly initialize\n",
    "            emb[i] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "\n",
    "    # This is our \"rare word\" id - we want to randomly initialize\n",
    "    emb[-1] = np.random.normal(scale=0.6, size=(n_fact,))\n",
    "    emb/=3\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single conv layer with max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CNN is likely to work better, since it's designed to take advantage of ordered data. We'll need to use a 1D CNN, since a sequence of words is 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1 = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len, dropout=0.2),\n",
    "    Dropout(0.2),\n",
    "    Convolution1D(64, 5, border_mode='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhoward/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 7s - loss: 0.5097 - acc: 0.7169 - val_loss: 0.2984 - val_acc: 0.8807\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 5s - loss: 0.3004 - acc: 0.8816 - val_loss: 0.2687 - val_acc: 0.8900\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 5s - loss: 0.2576 - acc: 0.9006 - val_loss: 0.2611 - val_acc: 0.8931\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 5s - loss: 0.2406 - acc: 0.9058 - val_loss: 0.2605 - val_acc: 0.8916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f728cc8b1d0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=4, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's past the Stanford paper's accuracy - another win for CNNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1.save_weights(model_path + 'conv1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1.load_weights(model_path + 'conv1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass our embedding matrix to the Embedding constructor, and set it to non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.2, \n",
    "              weights=[emb], trainable=False),\n",
    "    Dropout(0.25),\n",
    "    Convolution1D(64, 5, border_mode='same', activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 4s - loss: 0.6052 - acc: 0.6562 - val_loss: 0.4928 - val_acc: 0.7934\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 4s - loss: 0.4990 - acc: 0.7644 - val_loss: 0.4706 - val_acc: 0.7873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72879f1f98>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have beaten our previous model! But let's fine-tune the embedding weights - especially since the words we couldn't find in glove just have random embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 4s - loss: 0.2751 - acc: 0.8911 - val_loss: 0.2500 - val_acc: 0.9008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0de0c4e0d0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, that's given us a nice little boost. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'glove50.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-size CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of a multi-size CNN as shown in Ben Bowles' [excellent blog post](https://quid.com/feed/how-quid-uses-deep-learning-with-small-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the functional API to create multiple conv layers of different sizes, and then concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_in = Input ((vocab_size, 50))\n",
    "convs = [ ] \n",
    "for fsz in range (3, 6): \n",
    "    x = Convolution1D(64, fsz, border_mode='same', activation=\"relu\")(graph_in)\n",
    "    x = MaxPooling1D()(x) \n",
    "    x = Flatten()(x) \n",
    "    convs.append(x)\n",
    "out = Merge(mode=\"concat\")(convs) \n",
    "graph = Model(graph_in, out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then replace the conv/max-pool layer in our original CNN with the concatenated conv layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential ([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.2, weights=[emb]),\n",
    "    Dropout (0.2),\n",
    "    graph,\n",
    "    Dropout (0.5),\n",
    "    Dense (100, activation=\"relu\"),\n",
    "    Dropout (0.7),\n",
    "    Dense (1, activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 11s - loss: 0.3997 - acc: 0.8207 - val_loss: 0.3032 - val_acc: 0.8943\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 11s - loss: 0.2882 - acc: 0.8832 - val_loss: 0.2646 - val_acc: 0.9029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f55b79b7990>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, I found that in this case I got best results when I started the embedding layer as being trainable, and then set it to non-trainable after a couple of epochs. I have no idea why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 11s - loss: 0.2556 - acc: 0.8949 - val_loss: 0.2534 - val_acc: 0.9024\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 11s - loss: 0.2360 - acc: 0.9057 - val_loss: 0.2577 - val_acc: 0.9036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f55b74de110>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This more complex architecture has given us another boost in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We haven't covered this bit yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_13 (Embedding)         (None, 500, 32)       160064      embedding_input_13[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                   (None, 100)           53200       embedding_13[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 1)             101         lstm_13[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 213365\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len, mask_zero=True,\n",
    "              W_regularizer=l2(1e-6), dropout=0.2),\n",
    "    LSTM(100, consume_less='gpu'),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 100s - loss: 0.5007 - acc: 0.7446 - val_loss: 0.3475 - val_acc: 0.8531\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 100s - loss: 0.3524 - acc: 0.8507 - val_loss: 0.3602 - val_acc: 0.8453\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 99s - loss: 0.3750 - acc: 0.8342 - val_loss: 0.4758 - val_acc: 0.7710\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 99s - loss: 0.3238 - acc: 0.8652 - val_loss: 0.3094 - val_acc: 0.8725\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 99s - loss: 0.2681 - acc: 0.8920 - val_loss: 0.3018 - val_acc: 0.8776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9a16b12c50>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
